{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "model2.py",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMlqozDs7fWzWNILi0EUDlB",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/hamakim94/CNN_Project/blob/master/model2_py.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_BYhhOD3EoAw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "from konlpy.tag import Okt\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "import functions as fc\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "ko_model= Word2Vec.load('/content/drive/My Drive/ CNN_project/word2vec_movie.model') # word2vec 모델 로드\n",
        "\n",
        "# train, test 데이터 불러오기\n",
        "import pickle\n",
        "import pandas as pd\n",
        "\n",
        "test = pd.read_pickle(\"/content/drive/My Drive/ CNN_project/token_test_data.pkl\")\n",
        "train = pd.read_pickle(\"/content/drive/My Drive/ CNN_project/token_train_data.pkl\")\n",
        "\n",
        "training_sentences, training_labels = train['tokens'], train['labels']\n",
        "testing_sentences, testing_labels = test['tokens'], test['labels']\n",
        "\n",
        "vocab_size = 20000\n",
        "embedding_dim = 200\n",
        "max_length = 30\n",
        "truct_type = 'post'\n",
        "padding_type = 'post'\n",
        "oov_tok = '<OOV>'\n",
        "\n",
        "import functions as fc\n",
        "training_padded = fc.token_padded(training_sentences)\n",
        "testing_padded = fc.token_padded(testing_sentences)\n",
        "\n",
        "# making embedding matrix\n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "size = len(tokenizer.word_index) + 1\n",
        "embedding_matrix = np.zeros((size, embedding_dim))\n",
        "\n",
        "for word, idx in tokenizer.word_index.items():\n",
        "    embedding_vector = ko_model[word] if word in ko_model else None\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[idx] = embedding_vector\n",
        "\n",
        "embedding_dim = 200\n",
        "filter_sizes = (3, 4, 5)\n",
        "num_filters = 100\n",
        "dropout = 0.5\n",
        "hidden_dims = 100\n",
        "\n",
        "batch_size = 50\n",
        "num_epochs = 10\n",
        "min_word_count = 1\n",
        "context = 10\n",
        "\n",
        "conv_blocks =[]\n",
        "\n",
        "input_shape = (30)\n",
        "model_input = tf.keras.layers.Input(shape=input_shape)\n",
        "z = model_input\n",
        "for sz in filter_sizes:\n",
        "    embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length,\n",
        "                                         weights = [embedding_matrix], trainable = False)(z)\n",
        "    conv = tf.keras.layers.Conv1D(filters=num_filters,\n",
        "                         kernel_size=sz,\n",
        "                         padding=\"valid\",\n",
        "                         activation=\"relu\",\n",
        "                         strides=1)(embedding)\n",
        "    conv = tf.keras.layers.GlobalAveragePooling1D()(conv)\n",
        "    conv = tf.keras.layers.Flatten()(conv)\n",
        "    conv_blocks.append(conv)\n",
        "z = tf.keras.layers.Concatenate()(conv_blocks) if len(conv_blocks) > 1 else conv_blocks[0]\n",
        "\n",
        "z = tf.keras.layers.Dense(hidden_dims, activation=\"relu\", kernel_regularizer=tf.keras.regularizers.l2(0.003), bias_regularizer=tf.keras.regularizers.l2(0.003))(z)\n",
        "z = tf.keras.layers.Dropout(dropout)(z)\n",
        "model_output = tf.keras.layers.Dense(1, activation=\"sigmoid\")(z)\n",
        "model = tf.keras.Model(model_input, model_output)\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "import os #폴더 생성\n",
        "from tensorflow import keras\n",
        "\n",
        "checkpoint_dir = './ckpt2'\n",
        "if not os.path.exists(checkpoint_dir):\n",
        "    os.makedirs(checkpoint_dir)\n",
        "callbacks = [\n",
        "    keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=0),   \n",
        "    keras.callbacks.ModelCheckpoint(\n",
        "        filepath=checkpoint_dir + '/ckpt2-loss={loss:.3f}',\n",
        "        save_freq=500)\n",
        "]\n",
        "history = model.fit(training_padded, training_labels, epochs=10, callbacks=callbacks, batch_size = batch_size, validation_data=(testing_padded, testing_labels))\n",
        "\n",
        "plot_graphs(history, 'accuracy',name='model2_accuracy')\n",
        "plot_graphs(history, 'loss',name='model2_loss')   \n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}